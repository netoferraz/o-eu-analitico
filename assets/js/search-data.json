{
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
        "post7": {
            "title": "acórdãos tribunal de contas da união",
            "content": "Os acórdãos do Tribunal de Contas da União (TCU) são as decisões do órgão colegiado do tribunal. Assim, devido a repercusão geral desses acórdãos, decidi programar um crawler para coletar esses dados a partir do portal do TCU. O conteúdo do dataset diz respeito aos acórdãos entre os anos de 1992 até 30/08/2019. . Ao contrário da base original onde são apresentados os números de CPF em sua íntegra, o presente trabalho decidiu inserir uma máscara em toda ocorrência de CPF. Assim, estes possuem os 3 primeiros e os dois últimos dígitos mascarados, tratamento idêntico ao adotado pelo Portal da Transparência do Executivo Federal. . O dataset está publicado no kaggle : . . Além disso, caso tenham interesse no código de coleta dos dados o repositório é o: . . E aí gostaram? Comente aí ou deixe alguma sugestão! .",
            "url": "https://netoferraz.github.io/o-eu-analitico/dados%20abertos/governo/crawler/2019/09/07/acervotcu.html",
            "relUrl": "/dados%20abertos/governo/crawler/2019/09/07/acervotcu.html",
            "date": " • Sep 7, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "py-lexml-acervo",
            "content": "Você conhece o projeto LexML? O projeto visa unificar a informação legislativa e jurídica em um único portal. Eu escrevi um código que facilita a consulta à API do catálogo do LexML. Tá esperando o que para explorar esses dados? . . Além disso, fazendo uso do py-lexml-acervo disponibilizei um dataset contendo os metadados dos documentos publicados entre os anos de 1556 a 2019. O dataset possui aproximadamente 2GB compactado, e descompactado em torno de 10GB. . . Espero que tenham gostado! Qualquer dúvida ou sugestões deixem um comentário! .",
            "url": "https://netoferraz.github.io/o-eu-analitico/dados%20abertos/governo/crawler/2019/08/16/py_lexml_acervo.html",
            "relUrl": "/dados%20abertos/governo/crawler/2019/08/16/py_lexml_acervo.html",
            "date": " • Aug 16, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "dados: como classificá-los?",
            "content": "/o-eu-analitico/images/posts/ A grande maioria dos mapas temáticos faz uso de algum método para classificação de dados. O objetivo desse processo é organizar os dados em intervalos de dados ou em classes. A classificação se torna necessária para evitar um nível de granularidade muito elevado tornando a informação incompreensiva. Por outro lado, como apresentado em How to lie with maps um agrupamento incorreto pode distorcer a informação. Atualmente, existem programas (QGIS, ArcGIS, GeoPandas, etc.) que automatizam a produção de mapas com várias configurações default para “facilitar” a vida do usuário. Todavia, tais facilidades podem esconder armadilhas para usuários menos experientes. . Dados podem ser facilmente distorcidos quando analistas sucumbem ao senso de seguir as sugestões padrão das soluções de software. Por exemplo, a maior parte das vezes que um software gera um mapa coroplético a classificação utilizada nos dados é realizada por uma segmentação em 5 categorias baseada em intervalos iguais ou por quantis. (Livre Tradução) Lying with Maps, Mark Monmonier, Statistical Science, 2005, Vol. 20, No. 3, 215–222 . Em termos técnicos, o objetivo da classificação de dados é distribuir um conjunto de dados em N categorias, de modo a minimizar a variância dentro dos grupos e maximizá-la entre os grupos. Por exemplo, considere o conjunto de dados a seguir, que representa leituras fictícias de pH. . Iniciamos o processo com a determinação do número de classes (N) e com a escolha do método de categorização. A definição de N está associado a pergunta: O quanto de generalização você deseja em seus dados? Quanto maior for N menor será a generalização associada aos dados, todavia, muitas classes poderão implicar em problemas de legibilidade. Dessa forma, o recomendado é algo em torno de 3 a 7 classes. De sorte que, como regra de ouro, um bom início é avaliar o uso de 5 classes. Quanto ao método iremos utilizar o de intervalo iguais. Assim, calculamos a amplitude (A), para pH temos uma escala adimensional entre 0 e 14, portanto, A = 14. Para calcular o intervalo intra classe: A/N=2.8. Assim, concluímos a definição da distribuição dos dados em diferentes intervalos. . Uma das características desse método é a criação de classes com tamanho iguais. Todavia, quase sempre haverá uma distribuição desbalanceada dos dados nas diferentes categorias, podendo ocorrer alguma distorção por viés amostral em alguma classe, por exemplo. Por outro lado, a decisão de como realizar essa estratificação dos dados pode levar em conta o contexto da análise. O pH é uma medida utilizada em diversas áreas profissionais para mensurar o quão alcalino ou ácido um determinado meio é, como bem apresenta a ilustração abaixo do Guia do Estudante. . Vamos imaginar que você trabalhe em um órgão ambiental estadual que deve controlar o pH da água do mar das praias de uma cidade. É sabido que a água do mar é levemente alcalina, com pH entre 7,4 a 8,5. Além disso, vamos supor que a série histórica apresente uma média de 7.5 ± 1.2. Uma categorização pode ser efetuada usando-se uma medida de tendência central em conjunto com uma medida de dispersão, como a média e o desvio-padrão (DP), respectivamente. Dessa forma, criamos intervalos contextualizados aos dados. De modo que seja possível avaliar se as leituras de pH estão coerentes com o target, isto é, a média da série histórica. . A escolha do método de classificação influenciará o trabalho do analista. Ao ponto que histórias distintas podem ser contadas a partir de um mesmo conjunto de dados. Por exemplo, na figura abaixo são mostradas as escalas de cores produzidas por dois métodos distintos. Assim, são nítidas as mudanças de classificação. Portanto, histórias diferentes podem ser contadas a partir dessas escolhas. . Havendo uma compreensão inicial do que são métodos de classificação de dados e o seu objetivo, podemos explorar com maiores detalhes as principais abordagens existentes. Dessa forma, é válido ressaltar que não há uma única maneira de determinar um número de classes ideal e tampouco um melhor método. Assim, o caminho mais seguro é conhecer os prós e contras de cada abordagem e verificar se ela é apropriada ao seu conjunto de dados em particular. . Achou que iria ser molezinha? Eu também! Mas vamos lá, que não é tão complicado quanto pode parecer. Nesse post, vamos detalhar 4 dos classificadores mais utilizados: . Intervalos iguais (Equal Interval) | Quebras naturais (Natural Breaks) | Quantis (Quantiles) | Customizado (User Defined) | A Análise Exploratória de Dados (AED), disseminada pelo cientista John Tukey, permanece importante nos dias atuais. Várias das técnicas propostas, a serem realizadas durante o processo de AED, nos auxiliarão na tomada de decisão de qual método de classificação é mais apropriado para os seus dados. Para uma melhor compreensão do tópico, convoquei dois personagens que irão me auxiliar a melhor apresentar o assunto. São eles: Seu Creysson e Bino. Os dois atuarão como facilitadores de aprendizagem, em que o Seu Creysson sempre fará os apontamentos que são considerados corretos, enquanto o Bino se encarregará de indicar onde estão as ciladas. . 1. intervalos iguais (equal interval) . Como mencionado, serão nossos aliados a AED, a compreensão da distribuição dos dados - por meio de histogramas e de outras visualizações - e o entedimento do assunto estudado. . O método de intervalos iguais realiza uma divisão em classes com intervalos de mesmo tamanho, sendo mais apropriado a dados que se distribuam de forma semelhante por toda a amplitude do dataset. A seguir, temos como exemplo a categorização de um conjunto de dados arbitrário, que possui uma amplitude 100 unidades. Efetuamos a divisão do conjunto em 5 classes, com 20 unidades associadas a cada classe. . O conhecimento sobre as mais diversas distribuições de probabilidade será um aliado importante na tomada de decisão de como classificar os dados. O método de intervalos iguais se apresenta como uma boa opção para dados que se apresentem como uma distribuição uniforme. Todavia, se os dados apresentarem assimetria em alguma das direções ou sejam detectados pontos extremos (outliers), pode ser que alguma categoria fique vazia. Para melhor exemplificar esse conceito, utilizei as bibliotecas Numpy - para gerar dados amostrados de diferentes distribuições- e a PySAL - para avaliar diferentes classificadores. . Ao gerar dois datasets, o primeiro a partir de uma distribuição uniforme e outro a partir de uma distribuição log-normal, foi realizado uma classificação por intervalos iguais com 5 classes. A figura abaixo, com auxílio dos nossos facilitadores de aprendizagem, ilustra bem os exemplos de casos adequados e inadequados de uso desse tipo de classificador. No histograma da esquerda, temos um padrão de dados bem distribuídos em torno de toda a amplitude, configurando o caso ideal de uso desse tipo de classificador. Ao contrário, no da direita, temos uma distriuição com uma assimetria positiva, concentrando a maior parte dos valores na primeira classe, além do fato de que uma das classes ficou sem nenhum valor associado. . Como vantagem desse tipo de classificação, temos legendas que são mais fáceis de interpretar para audiências não técnicas. No tocante à desvantagem, para dados com distribuições não uniformes, poderão haver agrupamentos desproporcionais em apenas uma ou duas classes. . 2. quebras naturais (natural breaks) . Esse método também é conhecido por Jenks - em homenagem ao inventor George Frederick Jenks - e se utiliza de um algoritmo para minimizar a variação em cada grupo. Assim, quando a informação for apresentada em um mapa, as cores tenderão a aparecer mais distribuídas ao longo de toda visualização. O processo de otimização passa por uma iteração em que o algoritmo busca uma minimização da variância dentro dos grupos (agrupando os semelhantes), e maximizando as diferenças entre os grupos (separando os distintos). Além disso, as fronteiras de delimitação das classes são demarcadas nas descontinuidades da distribuição de dados. Portanto, esse método é indicado para ser utilizado em distribuições não normais e não uniformes. A seguir podemos listar algumas características desse método: . vantagens . Mapear valores que não são igualmente distribuídos ao longo da distribuição, resultando em classes mais balanceadas; | As classes geradas possuem o máximo possível de homogeniedade interna; | . desvantagens . O mapeamento em classes é inerente ao conjunto de dados em análise, portanto, há uma dificuldade em comparar mapas relacionados a datasets distintos; | Os resultados são bons apenas se existirem descontinuidades na distribuição de dados; | . Para representar o que foi explicado acima, foi utilizado o NumPy para criar um amostra de 1000 valores oriundos de uma distribuição binomial. Por conseguinte, foi realizada a classificação utilizando os métodos de quebras naturais e intervalos iguais com 5 classes. A ilustração abaixo apresenta o resultado desse comparativo, onde é possível atestar que a distribuição binomial apresenta algumas descontinuidades. Essa característica configura um dos requisitos de eficácia para o sucesso do algoritmo Jenks. Como resultado, é possível constatar uma distribuição mais homogênea dos valores entre as 5 classes no histograma da esquerda quando comparado ao da direita. . Os histogramas acima apresentam as diferenças entre os métodos comparados. Todavia, o impacto dessa classificação para dados apresentados em um mapa é ainda mais relevante. Considere o Levantamento de Preços e de Margens de Comercialização de Combustíveis realizado pela Agência Nacional do Petróleo, Gás Natural e Biocombustíveis (ANP). Os mapas abaixo apresentam a variação acumulada do preço do gás de cozinha (GLP) - em base nominal - entre janeiro de 2013 a junho de 2019. Logo, quando os dados são classificados pelo método de quebras naturais temos um maior número de estados associados as duas últimas categorias. Assim, evidencia-se que estados do Norte e Nordeste tiveram aumentos percentuais maiores que as demais regiões do país. . 3. quantis (quantiles) . A classificação por quantis tenta alocar um mesmo número de observações por classe. Portanto, conjuntos de dados que estão distribuídos de forma homogênea ao longo de toda a distribuição se beneficiam desse tipo de abordagem. A seguir listamos algumas características desse método: . vantagens . É possível enfatizar as posições relativas a determinados valores. Por exemplo, quais observações pertencem aos 20% maiores valores do conjunto de dados? | . desvantagens . Valores que pertecem a uma mesma classe podem ser muito diferentes, particularmente, se os dados não estão distribuídos de forma homogênea ao longo da amplitude; | O contrário também pode acontecer, que é o caso de observações muitos próximas serem atribuídas a classes distintas. | . Para contornar as desvantagens relatadas acima, uma possível abordagem é alterar o número de classes. De modo a apresentar o uso desse método, foi gerada uma amostra com 1000 observações oriundas de uma distribuição rayleigh, alocando esses dados em 5 classes distintas, uma pelo método dos quantis e outra por intervalos iguais. A ilustração abaixo apresenta a comparação. Como esperado, o primeiro método distribuiu 200 observações por classe, enquanto o segundo concentra 67% dos valores em apenas duas categorias. . É importante realçar a diferença de abordagem desses dois métodos. A por quantis se propõe a alocar uma mesma quantidade de observações por classe, independentemente da amplitude dessas. Enquanto a segunda preza pelo estabelecimento de classes com intervalos de mesma grandeza, sem ter o compromisso com o quantitativo de observações vinculadas a cada uma delas. De fato, esse método pode gerar classes sem nenhuma observação vinculada. . A importância de realizar uma AED de modo a definir a melhor estratégia de classificação é apresentada a seguir. Usamos os dados do censo de 2010 para construir um mapa coroplético da população por município do estado de São Paulo. . A ilustração abaixo resume os processos realizados até o momento de plotar o mapa. A primeira etapa [a] consiste na construção de um strip plot para avaliar a distribuição ao longo de toda amplitude. Além disso, é possível constatar que os dados se espalham por mais de sete ordens de grandeza, onde a maior parte deles formam uma nuvem de sobreposições no ínicio da escala. Nesse domínio, destacam-se municípios muito populosos, como o caso de São Paulo. . Uma abordagem para analisar dados que possuem outliers extremos como os apresentados no gráfico [a] é fazer uma transformação para uma outra escala, por exemplo, a logarítimica. O gráfico [b] apresenta o resultado dessa transformação. Desse modo, a distribuição dos dados fica mais evidente, tornando possível identificar o município de Borá que possui o menor número de habitantes registrado naquele censo, com apenas 805 habitantes. Diante disso, foi possível aplicar um método de classificação, como o por quantis, como bem ilustra o histograma do gráfico [c]. Por fim, foi possível construir o mapa coroplético [d] com os municípios associados a intervalos populacionais mais adequados. . Demonstramos, até aqui, a importância da escolha adequada de um método de classificação para uma melhor apresentação dos dados. No caso da produção de mapas, os padrões espaciais apresentados serão dependentes do classificador utilizado. Portanto, padrões que existam em um determinado mapa poderão não existir em outro que tenha usado um classificador diferente. Para melhor compreender essas diferenças, observem a ilustração abaixo, que apresenta o uso de diferentes classificadores para um mesmo conjunto de dados. Assim, é possível constatar a diferença nos padrões de mapeamento quando são utilizadas técnicas diferentes. . 4. customizado (user defined) . Lembram que mencionei que o conhecimento da regra de negócio associada aos dados é importante? Pois bem: e se o analista julgar que nenhuma das abordagens tradicionais se aplica ao conjunto de dados em análise? Nesse momento, entrará em campo a expertise de negócio, tanto para customizar as fronteiras entre as classes, quanto para definir a melhor forma de distribuir os dados em intervalos customizados. Portanto, faça uso de todas as suas habilidades como analista e explore diversas técnicas - em seu cinto de utilidades de cientista de dados - para uma tomada de decisão mais assertiva. . Assim como proposto, apresentamos 4 abordagens de como classificar dados para construir melhores mapas. Além disso, esse post não esgota todas as opções de classificadores, mas acredito que consiste numa boa introdução a quem deseja compreender as nuances envolvidas na apresentação de dados espaciais. Àqueles que desejam procurar uma fonte de informação para aprofundar seus conhecimentos, indico o livro Designing Better Maps da autora Cynthia Brewer, a mesma que criou a excelente ferramenta Color Brewer. . Por fim, mas não menos importante, gostaria de agradecer aos meu amigos Roberto Mourão, Paulo Haddad, Fernando Barbalho&lt;/span&gt; e Romualdo Alves pelas valiosas contribuições a esse texto. Fica aqui o meu muito-obrigado! . E aí gostou do post? Tem alguma sugestão ou dúvida? Deixe um comentário e até a próxima! .",
            "url": "https://netoferraz.github.io/o-eu-analitico/data%20wrangling/eda/2019/07/29/clf_dados.html",
            "relUrl": "/data%20wrangling/eda/2019/07/29/clf_dados.html",
            "date": " • Jul 29, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "lifelogging, um estilo de vida.",
            "content": "Você conhece o termo lifelogging? Também conhecido por Quantified self, esse consiste em um movimento criado no início de 2007 por Gary Wolf e Kevin Kelly, editores da revista Wired, o qual promove a incorporação de tecnologia para aquisição de dados no cotidiano das pessoas. . Em tempos onde há uma explosão de gadgets1 que monitoram em tempo real desde qualidade de sono a indicadores bioquímicos do corpo humano, constrói-se campo fértil para produção de insights a partir dos dados coletados, sem mencionar todo o mercado que está se construindo em torno de wearable techonology, como apresenta um recente artigo da revista Forbes que destaca uma previsão para 2019 que sejam vendidos 85 milhões de smartwatches. . Nos últimos anos muito tem falado sobre a implementação de uma cultura data-driven nas organizações, o qual se cria um ecossistema profisisonal orientado por dados, além disso, com todos os recursos tecnológicos disponíveis atualmente, também é possível trazer para a sua vida pessoal benefícios atrelados a prática da coleta de dados pessoais. . Um blog chamado o eu analíticonão poderia começar sua trajetória de forma diferente, se não, apresentando os dados e fatos de uma vida de lifelogging. Diante das inúmeras possibilidades do que coletar sobre o nosso cotidiano, aqui vai uma sugestão: . Colete dados sobre algo que tenha importância para você e que esteja envolvido com algum aspecto de sua vida que você avalie que necessita ser aprimorado. Do contrário, todo e qualquer benefício possível do processo de coleta perderá, rapidamente, o propósito. . Esse post tem por propósito compartilhar a minha trajetória dos últimos 3 anos relacionada ao time tracking. . 1. prelúdio . Em 2014 minha primeira filha nasceu e tudo mudou, e com isso passei por algo muito similar ao relatado por Josh Kaufman na excelente TED Talk The first 20 hours. Em uma nova vida de paternidade e responsabilidades, o meu tempo era mais escasso e como eu poderia me adaptar? Naquele ano começava a me chamar atenção uma nova área que estava começando a emergir chamada data science. Além disso, em 2015 eu e minha família nos mudamos mais uma vez, dessa vez para Brasília, e novamente, a vida demorou a ser organizar. Dito isso, foram necessários 2 anos para eu começar a vislumbrar que a tecnologia poderia me auxiliar na minha gestão de tempo e assim conciliar tudo que era importante para mim. . 2. o início . Estamos em 2016, eu estava há quase 1 ano morando em uma nova cidade, bem como em um novo emprego. Além disso, o interesse por data science só aumentava, mas eu ainda sentia que os estudos estavam demorando a engrenar, diante de uma rotina de trabalho integral, além da atenção necessária a família, eu julgava que me faltava tempo. . Em agosto daquele ano a Udacity abriu o seu escritório em São Paulo e começou a ofertar seus cursos precificados em Real, diante disso tomei a decisão de me inscrever no Nanodegree Data Analyst e essa decisão representa o meu tipping point para uma vida data-driven. Após uma pesquisa sobre gerenciamento de tempo, descobri algumas opções de softwares para time tracking, foi quando decidi adotar o toggl, uma solução mutiplataforma para gerenciamento de tempo. Além da gestão, essa ferramenta me auxiliou na construção de um novo hábito que deveria ser incorporado a minha rotina para que pudesse ser bem sucedido no nanodegree. . Finalmente, podemos dar um salto temporal para o presente e analisar os meus dados coletados pelo toggl e as implicações disso para a minha vida data-driven. . 3. lifelogging . O toggl permite, facilmente, a exportação de todo o registro para algum formato tabular, por exemplo, csv. Apesar do software fornecer várias visualizações prontas, bem como relatórios, a possibilidade de exportar o dado permite ao usuário extrapolar as funcionalidades do app. . Portanto, bastou carregar os dados no pandas para eu começar o processo de análise exploratória dos dados (EDA). Para um primeiro overview de uma série temporal registrando tempos dedicados a determinadas atividades, a visualização abaixo nos ajuda a ver o big picture desse conjunto de dados. . Para produção dos calendar heatmap fiz uso da biblioteca calmap que possui uma API bem simples e possui uma boa integração com os dataframes do pandas. . Pela escala de cores é possível identificar que o pico de registro é de no máximo umas 10 horas diárias e que em 2016 foi um período de incorporação do uso do aplicativo na minha rotina e que fica evidente que nos anos de 2017 e 2018 o hábito do time tracking já estava incorporado em minha vida. Além disso, essa visualização permite confirmar algo que sempre me propus: os finais de semana são períodos de descanso e dedicados a família. . Mapas de calor são eficientes para identificar trends, todavia, não são apropriados para extrair valores pontuais. Para tanto, o uso de um gráfico de barras atende melhor a esse propósito. Assim, pode-se constatar que os dias de início da semana são aqueles onde há um maior engajamento de lifelogging. . Já apresentamos evidências da consolidação de um novo hábito e que ele ocorre, prioritariamente, nos dia de semana. Mas ainda não falamos sobre o que eu decidi monitorar. A minha adesão ao lifelogging foi motivada, inicialmente, para organização do meu cotidiano para que fosse possível organizar meus estudos em data science, mas rapidamente foi extendida para um monitoramento das minhas atividades profissionais e aos poucos também incorporei o registro do tempo dedicado a atividade física, por exemplo. O gráfico abaixo, consolida esses dados. . A imagem revela que a maior parte do meu tempo registrado está ligada as minhas atividades profissionais, como tenho uma jornada de trabalho em tempo integral é razoável imaginar que ela ocuparia a maior parte do meu tempo de registro. Além da consolidação do tempo total, é possível explorar algumas características a respeito da minha produtividade ao longo de um dia de trabalho. . A construção de uma distribuição do total de horas trabalhadas ao longo de diferentes horas do dia revelou um histograma com dois picos: um no horário matutino entre as 09h:00min e 10h:59min da manhã e outro no vespertino entre as 14h:00min e 15h:59min. Além disso, os dados parecem se organizar em torno de algo próximo a uma distribuição bimodal. . Os dados me proporcionam a clareza necessária para saber em quais momentos ao longo do dia sou mais produtivo, portanto, aquelas reuniões clássicas que poderiam ser resolvidas em um email, eu tenho a condição de negociar para horários que não sou tão produtivo. . Working Hard GIF from Typing GIFs Abordado alguns padrões relacionados a minha rotina de trabalho, podemos explorar alguns aspectos da minha trajetória de incorporação de conhecimentos ligados a área de data science. Essa é uma área que tem despertado muito interesse nos últimos anos, e ao meu ver, a maior característica dela é a sua intrínseca interdisciplinaridade, e nada mais icônico para demonstrar isso que o diagrama de venn que mostra a intersecção entre conhecimentos das áreas de matemática/estatística &amp; computação &amp; área de negócio. Compartilho com vocês, uma citação muito famosa que considero adequada para representar esse profissional: . Um cientista de dados sabe mais de estatística que um cientista da computação e mais programação que um estatístico. . Desse modo, para organizar minha trajetória de aprendizado, comecei a registrar os meus estudos teóricos e projetos práticos nas áreas de ciência de dados, programação e desenvolvimento web. Além disso, em meados de 2017 eu descobri o incrível mundo dos podcasts e encontrei nessa mídia uma janela de oportunidade para adquirir conhecimentos ligados a área de ciência de dados, tecnologia, além de incorporar ao cotidiano maior exposição ao inglês (voltaremos a esse ponto, mais a frente). . O gráfico de área empilhada mostra as três categorias de aprendizado prioritárias, além disso, eu selecionei o tempo investido em podcasts ligados a área de ciência de dados ou de tecnologia da informação. No início da minha trajetória de lifelogging foi evidente o tempo investido em me aprofundar em assuntos ligados a área de data science e quanto mais eu aprendia, mas era perceptível que era necessário investir tempo em outras frentes, principalmente as ligadas a programação, tais como design pattern, testes, integração contínua, banco de dados, versionamento de código, entre outras (a lista é longa!). . O gráfico de pizza abaixo mostra que mais de 70% do tempo que fiz uso desse recurso foi para assuntos ligados a ciência de dados e tecnologia, os dados mostram que falta diversidade de conteúdo, certo? Mas é proposital devido ao ambiente de imersão que criei para acelerar meu aprendizado numa nova área do conhecimento. . Gostaria de destacar a importância que credito aos podcasts nessa experiência de aprendizado profissional e pessoal, caso você ainda não conheça, não perca tempo e incorpore na sua rotina. Dito isso, apresento a listagem dos programas mais populares na minha playlist divididos por assunto. . Por último, e não menos importante, para qualquer pessoa que pretenda ingressar na área de data science ou mesmo de tecnologia é imprescindível o conhecimento de inglês. . Você precisa criar o ambiente necessário em sua vida para a concretização dos seus sonhos, portanto, os podcasts podem ser mais uma maneira de incorporar o contato com um idioma estrangeiro no seu dia a dia. . O gráfico de barras empilhadas mostra que, a exceção de Julho de 2017, os podcasts em inglês sempre estão presentes de forma significativa na minha rotina, colaborando no mínimo, em duas frentes: o assunto técnico e o idioma estrangeiro. . Assim, acredito que abordei diversos aspectos ligados a construção de uma vida data-driven. Além disso, você já experimentou alguma experiência de lifelogging ou tem algum comentário sobre a minha experiência? Deixe o seu comentário e até a próxima postagem! .",
            "url": "https://netoferraz.github.io/o-eu-analitico/life%20logging/2019/02/07/lifelogging.html",
            "relUrl": "/life%20logging/2019/02/07/lifelogging.html",
            "date": " • Feb 7, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "sobre",
          "content": "Para maiores informações acessar o meu github :octocat: . .",
          "url": "https://netoferraz.github.io/o-eu-analitico/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://netoferraz.github.io/o-eu-analitico/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}