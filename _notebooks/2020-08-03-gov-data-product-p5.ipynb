{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"um produto orientado a dados governamentais: parte 5\"\n",
    "> \"fastapi, docker, cloud e tudo o mais para o deploy.\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [deep learning, nlp, data product, fastapi, docker, cloud, azure]\n",
    "- image: images/posts/govdata_poc_5/govdata_poc_minor_p5.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse √© o sexto post de uma s√©rie de como construir um produto data-driven de ponta a ponta, caso voc√™ ainda n√£o tenha acompanhado os demais, abaixo segue uma s√≠ntese com os respectivos links üòÄ.\n",
    "1. Em [metadados de normas jur√≠dicas federais]({{ site.baseurl }}{% link _posts/2020-07-07-metadados-normativos-federais.md %}) coletamos dados do sistema LexML.\n",
    "2. Em [um produto orientado a dados governamentais: parte 1]({{ site.baseurl }}{% link _posts/2020-07-12-gov-data-product.md%}) realizamos uma an√°lise explorat√≥ria dos dados e definimos um recorte e um escopo para os dados do projeto.\n",
    "3. Em [um produto orientado a dados governamentais: parte 2]({{ site.baseurl }}{% link _posts/2020-07-20-gov-data-product-p2.md%}) realizamos a defini√ß√£o dos dos datasets de treino, valida√ß√£o e teste\n",
    "4. Em [um produto orientado a dados governamentais: parte 3]({{ site.baseurl }}{% link _posts/2020-07-26-gov-data-product-p3.md%}) detalhamos tudo que n√£o deu certo no treinamento de modelos de machine learning.\n",
    "5. Em [um produto orientado a dados governamentais: parte 4]({{ site.baseurl }}{% link _posts/2020-07-30-gov-data-product-p4.md%}) apresentamos o treinamento de um modelo de deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dando continuidade ao nosso projeto, chegamos na etapa de *deploy* do [modelo treinado]({{ site.baseurl }}{% link _posts/2020-07-30-gov-data-product-p4.md%}). Os artefatos necess√°rios para colocar em produ√ß√£o o nosso modelo s√£o os seguintes arquivos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - O encoder multilabel utilizado no treinamento: [MultiLabelBinarizer](https://drive.google.com/uc?id=1-1K0jcHICzjgcaY64UeLC_PuEN5tI_Xa) \n",
    " - Os dados do modelo e vocabul√°rio do sentencepiece ([spm.model](https://drive.google.com/uc?id=1CyT0AI_PdWDZrnful6jBXFqAOfTrIOSe) e [spm.vocab](https://drive.google.com/uc?id=1bGetu3Uzq06OrtdvVmRfiY6uorVSayIS))\n",
    " - O modelo de classifica√ß√£o treinado ([trained_model_fp32_fwd_classifier.pkl](https://drive.google.com/uc?id=1R6Mm_K2ARMjNEuTikMmpHO0goggh0Rg9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√â importante ressaltar que o nosso treinamento foi realizado com GPU fazendo uso de [half-precision](https://forums.fast.ai/t/mixed-precision-training/20720) (fp16) e a m√°quina que iremos realizar o deploy n√£o ser√° alocado GPU, portanto, a infer√™ncia ser√° realizada com CPU. Assim, foi necess√°rio converter o modelo para [fp32](https://docs.fast.ai/basic_train.html#to_fp32), o que fez o tamanho do nosso modelo dobrar de tamanho (176M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dito isso, criamos um [reposit√≥rio](https://github.com/netoferraz/backend_datagovprod) no github para a api do nosso projeto. A pr√≥xima etapa √© definir em qual framework ser√° constru√≠da a api e depois de avaliar algumas das op√ß√µes dispon√≠veis foi decidido fazer uso da [fastapi](https://fastapi.tiangolo.com/), que √© um projeto muito parecido com [flask](https://flask.palletsprojects.com/en/1.1.x/) s√≥ que incorpora v√°rios benef√≠cios do uso de [type annotation](https://docs.python.org/3/library/typing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura inicial do *backend* est√° definida abaixo: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "|____processor\n",
    "| |____encoder.py\n",
    "| |____sp.py\n",
    "|____.gitignore\n",
    "|____main.py\n",
    "|____inference\n",
    "| |____predict.py\n",
    "|____LICENSE\n",
    "|____README.md\n",
    "|____models\n",
    "| |____model.py\n",
    "| |____download.py\n",
    "|____artifacts\n",
    "|____requirements.txt\n",
    "|____.env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir as depend√™ncias do projeto no `requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "fastai==1.0.61\n",
    "fastapi==0.60.1\n",
    "uvicorn==0.11.8\n",
    "sentencepiece==0.1.91\n",
    "scikit-learn==0.22.2\n",
    "numpy==1.19.1\n",
    "python-dotenv==0.14.0\n",
    "gdown==3.12.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, iremos definir algumas vari√°veis de ambiente para o projeto (`.env`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "artifactsPath=./artifacts/\n",
    "modelFileName=trained_model_fp32_fwd_classifier.pkl\n",
    "mlbinarizerFileName=onehot.pkl\n",
    "spmodelFileName=spm.model\n",
    "spmodelVocabFileName=spm.vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ambiente computacional ([colab](https://colab.research.google.com/)) utilizado para o treinamento do nosso classificador n√£o ser√° o mesmo do deploy. Portanto, na inst√¢ncia do objeto [Learner](https://docs.fast.ai/basic_train.html#Learner) precisamos alterar o path do modelo e do vocabul√°rio do [Sentence Piece Processor](https://docs.fast.ai/text.data.html#SPProcessor). Portanto, vamos escrever um c√≥digo para realizar essa modifica√ß√£o (`./processor/sp.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import SPProcessor\n",
    "from fastai.basic_train import Learner\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _fix_sp_processor(\n",
    "    learner: Learner, sp_path: Path, sp_model: str, sp_vocab: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fixes SentencePiece paths serialized into the model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner\n",
    "        Learner object\n",
    "    sp_path\n",
    "        path to the directory containing the SentencePiece model and vocabulary files.\n",
    "    sp_model\n",
    "        SentencePiece model filename.\n",
    "    sp_vocab\n",
    "        SentencePiece vocabulary filename.\n",
    "    \"\"\"\n",
    "    for processor in learner.data.processor:\n",
    "        if isinstance(processor, SPProcessor):\n",
    "            processor.sp_model = sp_path / sp_model\n",
    "            processor.sp_vocab = sp_path / sp_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos tamb√©m definir um c√≥digo para carregar a inst√¢ncia do [MultiLabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) usada no treinamento do modelo (`./processor/encoder.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\"\n",
    "load_dotenv()\n",
    "\n",
    "artifactsPath = Path(os.getenv(\"artifactsPath\"))\n",
    "mlBinarizerFileName = os.getenv(\"mlbinarizerFileName\")\n",
    "\n",
    "with open(artifactsPath / mlBinarizerFileName, \"rb\") as f:\n",
    "    onehot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos a decis√£o de n√£o subir para o reposit√≥rio os artefatos, principalmente, o classificador j√° que esse excedia os limites de tamanho permitidos pelo servi√ßo gratuito. Portanto, foi necess√°rio escrever uma rotina para baix√°-los (`./models/download.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "artifactsPath = './artifacts'\n",
    "if not os.path.exists(artifactsPath):\n",
    "    os.makedirs(artifactsPath)\n",
    "# one hot encoder\n",
    "if not os.path.isfile(\"./artifacts/onehot.pkl\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1-1K0jcHICzjgcaY64UeLC_PuEN5tI_Xa\")\n",
    "    os.rename(\"./onehot.pkl\", \"./artifacts/onehot.pkl\")\n",
    "\n",
    "# spm model\n",
    "if not os.path.isfile(\"./artifacts/spm.model\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1CyT0AI_PdWDZrnful6jBXFqAOfTrIOSe\")\n",
    "    os.rename(\"./spm.model\", \"./artifacts/spm.model\")\n",
    "\n",
    "# spm vocab\n",
    "if not os.path.isfile(\"./artifacts/spm.vocab\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1bGetu3Uzq06OrtdvVmRfiY6uorVSayIS\")\n",
    "    os.rename(\"./spm.vocab\", \"./artifacts/spm.vocab\")\n",
    "\n",
    "# trained_model_fp32_fwd_classifier\n",
    "if not os.path.isfile(\"./artifacts/trained_model_fp32_fwd_classifier.pkl\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1R6Mm_K2ARMjNEuTikMmpHO0goggh0Rg9\")\n",
    "    os.rename(\n",
    "        \"./trained_model_fp32_fwd_classifier.pkl\",\n",
    "        \"./artifacts/trained_model_fp32_fwd_classifier.pkl\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pr√≥ximo passo √© instanciar o nosso `modelo` e atualizar o path do `Processor` (`./models/model.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processor.sp import _fix_sp_processor\n",
    "from fastai.text import load_learner\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "artifactsPath = Path(os.getenv(\"artifactsPath\"))\n",
    "modelFileName = os.getenv(\"modelFileName\")\n",
    "spModel = os.getenv(\"spmodelFileName\")\n",
    "spVocab = os.getenv(\"spmodelVocabFileName\")\n",
    "\n",
    "model = load_learner(artifactsPath, modelFileName)\n",
    "_fix_sp_processor(model, artifactsPath, spModel, spVocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, podemos escrever a rota para consulta ao modelo (`./main.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.predict import predict\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Tuple\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Ementa(BaseModel):\n",
    "    ementa: str\n",
    "\n",
    "class Tags(BaseModel):\n",
    "    tags: Tuple[str, ...]\n",
    "\n",
    "\n",
    "@app.post(\"/predict\", response_model=Tags, status_code=200)\n",
    "def get_prediction(Ementa: Ementa):\n",
    "    ementa = Ementa.ementa\n",
    "\n",
    "    predictions = predict(ementa)\n",
    "\n",
    "    if not predictions:\n",
    "        raise HTTPException(\n",
    "            status_code=404, detail=\"N√£o foi poss√≠vel encontrar nenhuma tag apropriada.\"\n",
    "        )\n",
    "\n",
    "    if predictions:\n",
    "        return {\"tags\": predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos iniciar a aplica√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![local-backend](img/backend-gov-data-product/start-local-backend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos utilizar o `curl` para testar uma requisi√ß√£o e observar o retorno da `api`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![local-backend](img/backend-gov-data-product/curl-post-local-backend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excelente ü•≥! Temos o nosso modelo respondendo por meio da chamada de uma api. O pr√≥ximo passo √© dockerizar a aplica√ß√£o e subir o servi√ßo para produ√ß√£o üè≠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos reorganizar a nossa estrutura de diret√≥rios do projeto e criar um `Dockerfile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "|____app\n",
    "| |____processor\n",
    "| | |____encoder.py\n",
    "| | |____sp.py\n",
    "| |____.gitignore\n",
    "| |____main.py\n",
    "| |____inference\n",
    "| | |____predict.py\n",
    "| |____LICENSE\n",
    "| |____README.md\n",
    "| |____models\n",
    "| | |____model.py\n",
    "| | |____download.py\n",
    "| |____artifacts\n",
    "| |____requirements.txt\n",
    "| |____.env\n",
    "|____Dockerfile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura do `Dockerfile` est√° detalhada abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM python:3.7\n",
    "\n",
    "EXPOSE 8081\n",
    "\n",
    "COPY ./app /app\n",
    "WORKDIR /app\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "RUN python ./models/download.py\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8081\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos realizar o deploy da nossa api utilizando a [Azure](https://azure.microsoft.com/pt-br/). O primeiro passo √© fazer o [registro de uma imagem docker](https://azure.microsoft.com/pt-br/services/container-registry/) (*docker registry*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![docker-registry](img/backend-gov-data-product/register-docker-registry.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pr√≥xima etapa √© fazer o `build` da imagem `docker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![build-docker-image](img/backend-gov-data-product/build-docker-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para garantir que a aplica√ß√£o est√° funcionando adequadamente, vamos execut√°-la localmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run-docker-image](img/backend-gov-data-product/start-local-docker-backend-api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma realizada anteriormente, vamos realizar um `post` pelo `curl` e testar a `api`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test-api-docker](img/backend-gov-data-product/test-api-dockerizada.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtivemos a confirma√ß√£o de funcionamento da api no ambiente `docker`. Agora, podemos fazer o `push` da imagem para a azure. A primeira etapa √© realizar a autentica√ß√£o no servi√ßo, pelo seguinte comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker login pylegalclassifier.azurecr.io`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ser√° aberto um prompt solicitando login e senha que pode ser encontrado no painel de administra√ß√£o da aplica√ß√£o. Finalizado a autentica√ß√£o, vamos fazer o push da imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![push-docker-image](img/backend-gov-data-product/push-docker-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pr√≥xima etapa √© a cria√ß√£o de um [WebApp](https://azure.microsoft.com/pt-br/services/app-service/web/) no portal da Azure. Primeiramente, definimos o grupo de recursos o qual esse app far√° parte, em seguida definimos um nome para inst√¢ncia, bem como definimos que nossa aplica√ß√£o √© baseada em um container docker, e por √∫ltimo definimos a regi√£o onde ser√° alocado o recurso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![web-app-1](img/backend-gov-data-product/web-app-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, na aba `Docker` selecionamos a imagem que registramos no `docker registry`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![web-app-2](img/backend-gov-data-product/web-app-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ap√≥s confirmar todas informa√ß√µes, ser√° iniciado o `deploy` da nossa aplica√ß√£o e ap√≥s alguns minutos nossa api est√° em produ√ß√£o üöÄüöÄ!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![web-app-2](img/backend-gov-data-product/consulta-api-prod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"tenor-gif-embed\" data-postid=\"13347383\" data-share-method=\"host\" data-width=\"100%\" data-aspect-ratio=\"1.0774647887323943\"><a href=\"https://tenor.com/view/yes-baby-goal-funny-face-gif-13347383\">Yes Baby GIF</a> from <a href=\"https://tenor.com/search/yes-gifs\">Yes GIFs</a></div><script type=\"text/javascript\" async src=\"https://tenor.com/embed.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *framework* `fastapi` cria automaticamente uma documenta√ß√£o para suas apis, caso queiram visualizar esse recurso basta acessar [`https://pylegalclassifier.azurewebsites.net/docs`](https://pylegalclassifier.azurewebsites.net/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api-docs](img/backend-gov-data-product/api-docs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nosso deploy üñ•Ô∏è est√° conclu√≠do! Estamos nos aproximando do final dessa s√©rie de postagens. Espero que voc√™s estejam aproveitando tanto quanto eu üôã‚Äç‚ôÇÔ∏è Assim, no pr√≥ximo post iremos construir um exemplo de aplica√ß√£o que pode fazer uso da nossa `api`. At√© mais ü§ò!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "govdataprod",
   "language": "python",
   "name": "govdataprod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
